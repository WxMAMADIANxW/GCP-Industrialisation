{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "from google.cloud.storage import Client\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "import gcsfs\n",
    "import pickle\n",
    "from googleapiclient import discovery\n",
    "from oauth2client.client import GoogleCredentials\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Config work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXECUTE THE FOLLOWING COMMAND ONLY ONCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_TIME_VERSION  = datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT = PROJECT_ID[0]\n",
    "BUCKET_NAME = f\"{PROJECT}-machine-learning\"\n",
    "BUCKET= f\"gs://{PROJECT}-machine-learning\"\n",
    "RAW_DATA_FOLDER_NAME = \"raw-data\"\n",
    "RAW_DATA_FOLDER_PATH = f\"gs://{PROJECT}-machine-learning/raw-data\"\n",
    "ROOT='level-0-models'\n",
    "MODEL_DIR=os.path.join(ROOT,'models').replace(\"\\\\\",\"/\")\n",
    "PACKAGES_DIR=os.path.join(ROOT,'packages').replace(\"\\\\\",\"/\")\n",
    "REGION = 'europe-west1'\n",
    "MODEL_NAME = 'tweet_sentiment_classifier'\n",
    "\n",
    "if not os.path.exists('./model-'+ MODEL_TIME_VERSION +'/'):\n",
    "    os.makedirs('./model-'+ MODEL_TIME_VERSION +'/')\n",
    "temp_model = './model-'+ MODEL_TIME_VERSION +'/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "!gcloud config set project {PROJECT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a !gcloud storage command to create a bucket gs://{BUCKET_NAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud storage \"TO_FILL\" gs://{BUCKET_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the data from https://we.tl/t-g8SNeXnPh5 (cf description in https://www.kaggle.com/kazanova/sentiment140).\n",
    "You should get 4 files:\n",
    "- glove.twitter.27B.25d.txt\n",
    "- training_data.csv\n",
    "- dataset_VA.csv\n",
    "- dataset_VB.csv\n",
    "Go to Cloud Storage, bucket, and use the \"importer des fichiers\" or \"importer un dossier\" to upload those files into the bucket gs://{BUCKET_NAME}/raw-data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sentiment_mapping={\n",
    "    0:\"negative\",\n",
    "    2:\"neutral\",\n",
    "    4:\"positive\"\n",
    "}\n",
    "\n",
    "df_twitter = pd.read_csv(\"gs://\"+BUCKET_NAME+\"/raw-data/training_VA.csv\",encoding=\"latin1\", header=None)\\\n",
    "             .rename(columns={\n",
    "                 0:\"sentiment\",\n",
    "                 1:\"id\",\n",
    "                 2:\"time\",\n",
    "                 3:\"query\",\n",
    "                 4:\"username\",\n",
    "                 5:\"text\"\n",
    "             })[[\"sentiment\",\"text\"]]\n",
    "\n",
    "df_twitter[\"sentiment_label\"] = df_twitter[\"sentiment\"].map(sentiment_mapping)\n",
    "df_twitter.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a bar plot to visualize the distribution of sentiment_labels. Is the dataset balanced?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Data processing fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%%writefile preprocess.py\n",
    "\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing import text\n",
    "import re\n",
    "\n",
    "class TextPreprocessor(object):\n",
    "    def _clean_line(self, text):\n",
    "        text = re.sub(r\"http\\S+\", \"\", text)\n",
    "        text = re.sub(r\"@[A-Za-z0-9]+\", \"\", text)\n",
    "        text = re.sub(r\"#[A-Za-z0-9]+\", \"\", text)\n",
    "        text = text.replace(\"RT\",\"\")\n",
    "        text = text.lower()\n",
    "        text = text.strip()\n",
    "        return text\n",
    "    \n",
    "    def __init__(self, vocab_size, max_sequence_length):\n",
    "        self._vocab_size = vocab_size\n",
    "        self._max_sequence_length = max_sequence_length\n",
    "        self._tokenizer = None\n",
    "\n",
    "    def fit(self, text_list):        \n",
    "        # Create vocabulary from input corpus.\n",
    "        text_list_cleaned = [self._clean_line(txt) for txt in text_list]\n",
    "        tokenizer = text.Tokenizer(num_words=self._vocab_size)\n",
    "        tokenizer.fit_on_texts(text_list)\n",
    "        self._tokenizer = tokenizer\n",
    "\n",
    "    def transform(self, text_list):        \n",
    "        # Transform text to sequence of integers\n",
    "        text_list = [self._clean_line(txt) for txt in text_list]\n",
    "        text_sequence = self._tokenizer.texts_to_sequences(text_list)\n",
    "\n",
    "        # Fix sequence length to max value. Sequences shorter than the length are\n",
    "        # padded in the beginning and sequences longer are truncated\n",
    "        # at the beginning.\n",
    "        padded_text_sequence = sequence.pad_sequences(\n",
    "          text_sequence, maxlen=self._max_sequence_length)\n",
    "        return padded_text_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some small test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from preprocess import TextPreprocessor\n",
    "processor = TextPreprocessor(5, 5)\n",
    "processor.fit(['hello machine learning','test'])\n",
    "processor.transform(['hello machine learning',\"lol\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Prep data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wish to limit the size of the vocabulary used for tokenization to 25000.\n",
    "The sentences with be truncated to a length of 50 words if they are too long, and passed to 50 if they are too short. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CLASSES = {'negative':0, 'positive': 1}  # label-to-int mapping\n",
    "VOCAB_SIZE = \"TO FILL WITH AN INTEGER\"\n",
    "MAX_SEQUENCE_LENGTH = \"TO FILL WITH AN INTEGER\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sents = df_twitter.text\n",
    "labels = np.array(df_twitter.sentiment_label.map(CLASSES))\n",
    "\n",
    "# Train and test split\n",
    "X, X_test, y, y_test = train_test_split(sents, labels, test_size=0.2)\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "# Create vocabulary from training corpus.\n",
    "processor = TextPreprocessor(VOCAB_SIZE, MAX_SEQUENCE_LENGTH)\n",
    "processor.fit(X_train)\n",
    "\n",
    "# Preprocess the data\n",
    "train_texts_vectorized = processor.transform(X_train)\n",
    "eval_texts_vectorized = processor.transform(X_test)\n",
    "validation_texts_vectorized = processor.transform(X_validation)\n",
    "\n",
    "with open('./model-'+ MODEL_TIME_VERSION +'/processor_state.pkl', 'wb') as f:\n",
    "    pickle.dump(processor, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model parameters\n",
    "LEARNING_RATE=.001\n",
    "EMBEDDING_DIM=50\n",
    "FILTERS=64\n",
    "DROPOUT_RATE=0.5\n",
    "POOL_SIZE=3\n",
    "NUM_EPOCH=2\n",
    "BATCH_SIZE=128\n",
    "KERNEL_SIZES=[2,5,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Basic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fill in the missing variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def create_model(vocab_size, embedding_dim, filters, kernel_sizes, dropout_rate, pool_size, embedding_matrix):\n",
    "    \n",
    "    # Input layer\n",
    "    model_input = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "    # Embedding layer\n",
    "    z = tf.keras.layers.Embedding(\n",
    "        input_dim=vocab_size+1,\n",
    "        output_dim=\"TO FILL\",\n",
    "        input_length=MAX_SEQUENCE_LENGTH,\n",
    "        weights=[embedding_matrix]\n",
    "    )(model_input)\n",
    "\n",
    "    z = tf.keras.layers.Dropout(dropout_rate)(z)\n",
    "\n",
    "    # Convolutional block\n",
    "    conv_blocks = []\n",
    "    for kernel_size in kernel_sizes:\n",
    "        conv = tf.keras.layers.Convolution1D(\n",
    "            filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=\"valid\",\n",
    "            activation=\"relu\",\n",
    "            bias_initializer='random_uniform',\n",
    "            strides=1)(z)\n",
    "        conv = tf.keras.layers.MaxPooling1D(pool_size=2)(conv)\n",
    "        conv = tf.keras.layers.Flatten()(conv)\n",
    "        conv_blocks.append(conv)\n",
    "        \n",
    "    z = tf.keras.layers.Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\n",
    "\n",
    "    z = tf.keras.layers.Dropout(dropout_rate)(z)\n",
    "    z = tf.keras.layers.Dense(100, activation=\"relu\")(z)\n",
    "    model_output = tf.keras.layers.Dense(1, activation=\"sigmoid\")(z)\n",
    "\n",
    "    model = tf.keras.models.Model(model_input, model_output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Pretrained Glove embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding can be downloaded here: https://nlp.stanford.edu/projects/glove/, but you should already have the globe.twitter.27B.25d.txt file in your cloud storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the missing part to download the glove.twitter.27B.25d.txt from your cloud storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = Client()\n",
    "bucket = \"TO FILL\"\n",
    "blob = \"TO FILL\"\n",
    "downloaded_file = blob.download_to_filename('raw-data/glove.twitter.27B.25d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_coaefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coaefs(*o.strip().split()) for o in\n",
    "                                                open(\"raw-data/glove.twitter.27B.25d.txt\",\"r\",encoding=\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "word_index = processor._tokenizer.word_index\n",
    "nb_words = min(VOCAB_SIZE, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words + 1, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= VOCAB_SIZE: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've got an error? Fix it!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Create - compile - train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the missing arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model = create_model(VOCAB_SIZE, EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compile model with learning parameters.\n",
    "optimizer = tf.keras.optimizers.Nadam(lr=0.001)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#keras train\n",
    "history = model.fit(\n",
    "    train_texts_vectorized, \n",
    "    y_train, \n",
    "    epochs=NUM_EPOCH, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(validation_texts_vectorized, y_validation),\n",
    "    verbose=2,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_acc',\n",
    "            min_delta=0.005,\n",
    "            patience=3,\n",
    "            factor=0.5),\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            min_delta=0.005, \n",
    "            patience=5, \n",
    "            verbose=0, \n",
    "            mode='auto'\n",
    "        ),\n",
    "        tf.keras.callbacks.History()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model : acc loss\n",
    "[loss, acc] = model.evaluate(eval_texts_vectorized, y_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.predict(eval_texts_vectorized)\n",
    "predictions = np.array([int(np.round(i)) for i in scores ])\n",
    "confusion_matrix=tf.math.confusion_matrix(predictions, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "print (str(\"matrix-co : \"+str(confusion_matrix)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# std prediction \n",
    "np.std(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"history.pkl\",'wb') as file:\n",
    "    pickle.dump(history.history,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.keras.models.save_model(model,temp_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the missing command to copy the temp_model directory to gcp storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!\"TO FILL\" -r {temp_model} {BUCKET}/{MODEL_DIR}/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Prepare custom model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%%writefile model_prediction.py\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.lib.io import file_io\n",
    "\n",
    "class CustomModelPrediction(object):\n",
    "\n",
    "    def __init__(self, model, processor):\n",
    "        # Class gets instantiated with a trained model file and a persisted processor\n",
    "        self._model = model\n",
    "        self._processor = processor\n",
    "\n",
    "    def _postprocess(self, predictions):\n",
    "    # Create an output signature\n",
    "        labels = ['negative', 'positive']\n",
    "        return [\n",
    "            {\n",
    "            \"label\":labels[int(np.round(prediction))],\n",
    "            \"score\":float(np.round(prediction,4))\n",
    "            } for prediction in predictions]\n",
    "\n",
    "    def predict(self, instances, **kwargs):\n",
    "    # Clean the data, make predictions and postprocess\n",
    "        preprocessed_data = self._processor.transform(instances)\n",
    "        predictions =  self._model.predict(preprocessed_data)\n",
    "        labels = self._postprocess(predictions)\n",
    "        return labels\n",
    "\n",
    "    @classmethod\n",
    "    def from_path(cls, model_dir):\n",
    "    # Load the keras model and the persisted processor\n",
    "        \n",
    "        print ('test model')\n",
    "        model = tf.keras.models.load_model(model_dir,custom_objects={'tf': tf})\n",
    "    \n",
    "    # I know, pickle is bad and I should feel bad\n",
    "    \n",
    "        with file_io.FileIO(os.path.join(model_dir, 'processor_state.pkl'), 'rb') as f:\n",
    "            processor = pickle.load(f)\n",
    "\n",
    "        return cls(model, processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "requests = ([\"God I hate the north\",\"god I love this\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET+'/'+MODEL_DIR+temp_model[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from model_prediction import CustomModelPrediction\n",
    "\n",
    "classifier = CustomModelPrediction.from_path(BUCKET+'/'+MODEL_DIR+temp_model[1:])\n",
    "results = classifier.predict(requests)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Package it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_TIME_VERSION\n",
    "print(MODEL_TIME_VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### update VERSION in the cell below with the MODEL_TIME_VERSION above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%%writefile setup.py\n",
    "\n",
    "from setuptools import setup\n",
    "\n",
    "MODEL_NAME = \"tweet_sentiment_classifier\"\n",
    "REQUIRED_PACKAGES = ['gcsfs']\n",
    "VERSION = '2022-06-xx-xx-xx-xx'\n",
    "\n",
    "setup(\n",
    "    name=MODEL_NAME,\n",
    "    packages=[],\n",
    "    include_package_data=False,\n",
    "    version=VERSION,\n",
    "    scripts=[\"preprocess.py\", \"model_prediction.py\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrap it up and copy to GCP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the missing command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "!python setup.py sdist --formats=gztar\n",
    "!\"TO FILL\" ./dist/{MODEL_NAME}-{MODEL_TIME_VERSION}.tar.gz {BUCKET}/{PACKAGES_DIR}/{MODEL_NAME}-{MODEL_TIME_VERSION}.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've got an error? Try to fix it!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create model and version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "VERSION_NAME='V_' + MODEL_TIME_VERSION\n",
    "RUNTIME_VERSION='2.5' # tensorflow version\n",
    "MODEL_REGION='europe-west1'\n",
    "id_model = 'model-'+MODEL_TIME_VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "list your existing ai-platform model using a gcloud ai-platform command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud \"TO FILL\" --region global"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If no model has been created before, fill the next gcloud ai-platform command to create a model, then run this command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "!gcloud ai-platform \"TO FILL\" {MODEL_NAME} --regions {MODEL_REGION}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill the next command to create a new version of the model using beta ai-platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud \"TO FILL\" {VERSION_NAME} \\\n",
    "--model {MODEL_NAME} \\\n",
    "--origin {BUCKET}/{MODEL_DIR}/{id_model} \\\n",
    "--python-version 3.7 \\\n",
    "--runtime-version {RUNTIME_VERSION} \\\n",
    "--package-uris {BUCKET}/{PACKAGES_DIR}/{MODEL_NAME}-{MODEL_TIME_VERSION}.tar.gz \\\n",
    "--prediction-class=model_prediction.CustomModelPrediction \\\n",
    "--region global "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops, the fix you made just above might be useful again..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "requests = [\n",
    "    \"god this episode sucks\",\n",
    "    \"meh, I kinda like it\",\n",
    "    \"what were the writer thinking, omg it doesn't make any sense!\",\n",
    "    \"omg! what a twist, who would've though :o!\",\n",
    "    \"woohoow, sansa for the win!\"\n",
    "]\n",
    "\n",
    "# JSON format the requests\n",
    "request_data = {'instances': requests}\n",
    "\n",
    "# Authenticate and call CMLE prediction API \n",
    "credentials = GoogleCredentials.get_application_default()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "api = discovery.build('ml', 'v1')\n",
    "model_url = 'projects/{}/models/{}'.format(PROJECT, MODEL_NAME)\n",
    "response = api.projects().predict(body=request_data, name=model_url).execute()\n",
    "response[\"predictions\"]"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-3.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m108"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
