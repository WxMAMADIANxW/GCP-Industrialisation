{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "from google.cloud.storage import Client\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "import gcsfs\n",
    "import pickle\n",
    "from googleapiclient import discovery\n",
    "from oauth2client.client import GoogleCredentials\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Config work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXECUTE THE FOLLOWING COMMAND ONLY ONCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_TIME_VERSION  = datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT = PROJECT_ID[0]\n",
    "BUCKET_NAME = f\"{PROJECT}-machine-learning\"\n",
    "BUCKET= f\"gs://{PROJECT}-machine-learning\"\n",
    "RAW_DATA_FOLDER_NAME = \"raw-data\"\n",
    "RAW_DATA_FOLDER_PATH = f\"gs://{PROJECT}-machine-learning/raw-data\"\n",
    "ROOT='level-0-models'\n",
    "MODEL_DIR=os.path.join(ROOT,'models').replace(\"\\\\\",\"/\")\n",
    "PACKAGES_DIR=os.path.join(ROOT,'packages').replace(\"\\\\\",\"/\")\n",
    "REGION = 'europe-west1'\n",
    "MODEL_NAME = 'tweet_sentiment_classifier'\n",
    "\n",
    "if not os.path.exists('./model-'+ MODEL_TIME_VERSION +'/'):\n",
    "    os.makedirs('./model-'+ MODEL_TIME_VERSION +'/')\n",
    "temp_model = './model-'+ MODEL_TIME_VERSION +'/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "!gcloud config set project {PROJECT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data can be downloaded from: https://www.kaggle.com/kazanova/sentiment140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text  \\\n",
       "0          0  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
       "1          0  is upset that he can't update his Facebook by ...   \n",
       "2          0  @Kenichan I dived many times for the ball. Man...   \n",
       "3          0    my whole body feels itchy and like its on fire    \n",
       "4          0  @nationwideclass no, it's not behaving at all....   \n",
       "\n",
       "  sentiment_label  \n",
       "0        negative  \n",
       "1        negative  \n",
       "2        negative  \n",
       "3        negative  \n",
       "4        negative  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_mapping={\n",
    "    0:\"negative\",\n",
    "    2:\"neutral\",\n",
    "    4:\"positive\"\n",
    "}\n",
    "\n",
    "df_twitter = pd.read_csv(\"gs://\"+BUCKET_NAME+\"/raw-data/training_VA.csv\",encoding=\"latin1\", header=None)\\\n",
    "             .rename(columns={\n",
    "                 0:\"sentiment\",\n",
    "                 1:\"id\",\n",
    "                 2:\"time\",\n",
    "                 3:\"query\",\n",
    "                 4:\"username\",\n",
    "                 5:\"text\"\n",
    "             })[[\"sentiment\",\"text\"]]\n",
    "\n",
    "df_twitter[\"sentiment_label\"] = df_twitter[\"sentiment\"].map(sentiment_mapping)\n",
    "print(df_twitter.shape)\n",
    "df_twitter.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Data processing fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocess.py\n",
    "\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing import text\n",
    "import re\n",
    "\n",
    "class TextPreprocessor(object):\n",
    "    def _clean_line(self, text):\n",
    "        text = re.sub(r\"http\\S+\", \"\", text)\n",
    "        text = re.sub(r\"@[A-Za-z0-9]+\", \"\", text)\n",
    "        text = re.sub(r\"#[A-Za-z0-9]+\", \"\", text)\n",
    "        text = text.replace(\"RT\",\"\")\n",
    "        text = text.lower()\n",
    "        text = text.strip()\n",
    "        return text\n",
    "    \n",
    "    def __init__(self, vocab_size, max_sequence_length):\n",
    "        self._vocab_size = vocab_size\n",
    "        self._max_sequence_length = max_sequence_length\n",
    "        self._tokenizer = None\n",
    "\n",
    "    def fit(self, text_list):        \n",
    "        # Create vocabulary from input corpus.\n",
    "        text_list_cleaned = [self._clean_line(txt) for txt in text_list]\n",
    "        tokenizer = text.Tokenizer(num_words=self._vocab_size)\n",
    "        tokenizer.fit_on_texts(text_list)\n",
    "        self._tokenizer = tokenizer\n",
    "\n",
    "    def transform(self, text_list):        \n",
    "        # Transform text to sequence of integers\n",
    "        text_list = [self._clean_line(txt) for txt in text_list]\n",
    "        text_sequence = self._tokenizer.texts_to_sequences(text_list)\n",
    "\n",
    "        # Fix sequence length to max value. Sequences shorter than the length are\n",
    "        # padded in the beginning and sequences longer are truncated\n",
    "        # at the beginning.\n",
    "        padded_text_sequence = sequence.pad_sequences(\n",
    "          text_sequence, maxlen=self._max_sequence_length)\n",
    "        return padded_text_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some small test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 2, 3],\n",
       "       [0, 0, 0, 0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from preprocess import TextPreprocessor\n",
    "processor = TextPreprocessor(5, 5)\n",
    "processor.fit(['hello machine learning','test'])\n",
    "processor.transform(['hello machine learning',\"lol\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Prep data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CLASSES = {'negative':0, 'positive': 1}  # label-to-int mapping\n",
    "VOCAB_SIZE = 25000  # Limit on the number vocabulary size used for tokenization\n",
    "MAX_SEQUENCE_LENGTH = 50  # Sentences will be truncated/padded to this length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sents = df_twitter.text\n",
    "labels = np.array(df_twitter.sentiment_label.map(CLASSES))\n",
    "\n",
    "# Train and test split\n",
    "X, X_test, y, y_test = train_test_split(sents, labels, test_size=0.2)\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "# Create vocabulary from training corpus.\n",
    "processor = TextPreprocessor(VOCAB_SIZE, MAX_SEQUENCE_LENGTH)\n",
    "processor.fit(X_train)\n",
    "\n",
    "# Preprocess the data\n",
    "train_texts_vectorized = processor.transform(X_train)\n",
    "eval_texts_vectorized = processor.transform(X_test)\n",
    "validation_texts_vectorized = processor.transform(X_validation)\n",
    "\n",
    "with open('./model-'+ MODEL_TIME_VERSION +'/processor_state.pkl', 'wb') as f:\n",
    "    pickle.dump(processor, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model parameters\n",
    "LEARNING_RATE=.001\n",
    "EMBEDDING_DIM=25\n",
    "FILTERS=64\n",
    "DROPOUT_RATE=0.5\n",
    "POOL_SIZE=3\n",
    "NUM_EPOCH=2\n",
    "BATCH_SIZE=128\n",
    "KERNEL_SIZES=[2,5,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Basic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def create_model(vocab_size, embedding_dim, filters, kernel_sizes, dropout_rate, pool_size, embedding_matrix):\n",
    "    \n",
    "    # Input layer\n",
    "    model_input = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "    # Embedding layer\n",
    "    z = tf.keras.layers.Embedding(\n",
    "        input_dim=vocab_size+1,\n",
    "        output_dim=embedding_dim,\n",
    "        input_length=MAX_SEQUENCE_LENGTH,\n",
    "        weights=[embedding_matrix]\n",
    "    )(model_input)\n",
    "\n",
    "    z = tf.keras.layers.Dropout(dropout_rate)(z)\n",
    "\n",
    "    # Convolutional block\n",
    "    conv_blocks = []\n",
    "    for kernel_size in kernel_sizes:\n",
    "        conv = tf.keras.layers.Convolution1D(\n",
    "            filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=\"valid\",\n",
    "            activation=\"relu\",\n",
    "            bias_initializer='random_uniform',\n",
    "            strides=1)(z)\n",
    "        conv = tf.keras.layers.MaxPooling1D(pool_size=2)(conv)\n",
    "        conv = tf.keras.layers.Flatten()(conv)\n",
    "        conv_blocks.append(conv)\n",
    "        \n",
    "    z = tf.keras.layers.Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\n",
    "\n",
    "    z = tf.keras.layers.Dropout(dropout_rate)(z)\n",
    "    z = tf.keras.layers.Dense(100, activation=\"relu\")(z)\n",
    "    model_output = tf.keras.layers.Dense(1, activation=\"sigmoid\")(z)\n",
    "\n",
    "    model = tf.keras.models.Model(model_input, model_output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Pretrained Glove embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embedding can be downloaded here: https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "client = Client()\n",
    "bucket = client.get_bucket(BUCKET_NAME)\n",
    "temp_folder = \"raw-data/\"\n",
    "if not os.path.exists(temp_folder):\n",
    "    os.makedirs(temp_folder)\n",
    "blob = bucket.get_blob(\"raw-data/glove.twitter.27B.25d.txt\")\n",
    "downloaded_file = blob.download_to_filename('raw-data/glove.twitter.27B.25d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_coaefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coaefs(*o.strip().split()) for o in\n",
    "                                                open(\"raw-data/glove.twitter.27B.25d.txt\",\"r\",encoding=\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "word_index = processor._tokenizer.word_index\n",
    "nb_words = min(VOCAB_SIZE, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words + 1, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= VOCAB_SIZE: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Create - compile - train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-05 16:19:38.244710: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2299995000 Hz\n",
      "2022-06-05 16:19:38.245272: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55b2809415c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-06-05 16:19:38.245303: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2022-06-05 16:19:38.248090: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "model = create_model(VOCAB_SIZE, EMBEDDING_DIM, FILTERS, KERNEL_SIZES, DROPOUT_RATE,POOL_SIZE, embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compile model with learning parameters.\n",
    "optimizer = tf.keras.optimizers.Nadam(lr=0.001)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "9000/9000 - 739s - loss: 0.4408 - acc: 0.7948 - val_loss: 0.4170 - val_acc: 0.8083\n",
      "Epoch 2/2\n",
      "9000/9000 - 742s - loss: 0.4288 - acc: 0.8016 - val_loss: 0.4114 - val_acc: 0.8124\n"
     ]
    }
   ],
   "source": [
    "#keras train\n",
    "history = model.fit(\n",
    "    train_texts_vectorized, \n",
    "    y_train, \n",
    "    epochs=NUM_EPOCH, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(validation_texts_vectorized, y_validation),\n",
    "    verbose=2,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_acc',\n",
    "            min_delta=0.005,\n",
    "            patience=3,\n",
    "            factor=0.5),\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            min_delta=0.005, \n",
    "            patience=5, \n",
    "            verbose=0, \n",
    "            mode='auto'\n",
    "        ),\n",
    "        tf.keras.callbacks.History()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 100s 10ms/step - loss: 0.4092 - acc: 0.8152\n"
     ]
    }
   ],
   "source": [
    "# test model : acc loss\n",
    "[loss, acc] = model.evaluate(eval_texts_vectorized, y_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.predict(eval_texts_vectorized)\n",
    "predictions = np.array([int(np.round(i)) for i in scores ])\n",
    "confusion_matrix=tf.math.confusion_matrix(predictions, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix-co : tf.Tensor(\n",
      "[[132431  31322]\n",
      " [ 27829 128418]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix\n",
    "print (str(\"matrix-co : \"+str(confusion_matrix)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3225778"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# std prediction \n",
    "np.std(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"history.pkl\",'wb') as file:\n",
    "    pickle.dump(history.history,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-05 16:59:53.456952: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model-2022-06-05-13-38-45/assets\n"
     ]
    }
   ],
   "source": [
    "tf.keras.models.save_model(model,temp_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy file to gcp storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://./model-2022-06-05-13-38-45/processor_state.pkl [Content-Type=application/octet-stream]...\n",
      "Copying file://./model-2022-06-05-13-38-45/saved_model.pb [Content-Type=application/octet-stream]...\n",
      "Copying file://./model-2022-06-05-13-38-45/variables/variables.index [Content-Type=application/octet-stream]...\n",
      "Copying file://./model-2022-06-05-13-38-45/variables/variables.data-00000-of-00001 [Content-Type=application/octet-stream]...\n",
      "\\ [4 files][ 48.6 MiB/ 48.6 MiB]                                                \n",
      "Operation completed over 4 objects/48.6 MiB.                                     \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp -r {temp_model} {BUCKET}/{MODEL_DIR}/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Prepare custom model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model_prediction.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model_prediction.py\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.lib.io import file_io\n",
    "\n",
    "class CustomModelPrediction(object):\n",
    "\n",
    "    def __init__(self, model, processor):\n",
    "        # Class gets instantiated with a trained model file and a persisted processor\n",
    "        self._model = model\n",
    "        self._processor = processor\n",
    "\n",
    "    def _postprocess(self, predictions):\n",
    "    # Create an output signature\n",
    "        labels = ['negative', 'positive']\n",
    "        return [\n",
    "            {\n",
    "            \"label\":labels[int(np.round(prediction))],\n",
    "            \"score\":float(np.round(prediction,4))\n",
    "            } for prediction in predictions]\n",
    "\n",
    "    def predict(self, instances, **kwargs):\n",
    "    # Clean the data, make predictions and postprocess\n",
    "        preprocessed_data = self._processor.transform(instances)\n",
    "        predictions =  self._model.predict(preprocessed_data)\n",
    "        labels = self._postprocess(predictions)\n",
    "        return labels\n",
    "\n",
    "    @classmethod\n",
    "    def from_path(cls, model_dir):\n",
    "    # Load the keras model and the persisted processor\n",
    "        \n",
    "        print ('test model')\n",
    "        model = tf.keras.models.load_model(model_dir,custom_objects={'tf': tf})\n",
    "    \n",
    "    # I know, pickle is bad and I should feel bad\n",
    "    \n",
    "        with file_io.FileIO(os.path.join(model_dir, 'processor_state.pkl'), 'rb') as f:\n",
    "            processor = pickle.load(f)\n",
    "\n",
    "        return cls(model, processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "requests = ([\"God I hate the north\",\"god I love this\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://sound-splicer-351114-machine-learning/level-0-models/models/model-2022-06-05-13-38-45/'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUCKET+'/'+MODEL_DIR+temp_model[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'negative', 'score': 0.03759999945759773},\n",
       " {'label': 'positive', 'score': 0.9294000267982483}]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model_prediction import CustomModelPrediction\n",
    "\n",
    "classifier = CustomModelPrediction.from_path(BUCKET+'/'+MODEL_DIR+temp_model[1:])\n",
    "results = classifier.predict(requests)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Package it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2022-06-05-13-38-45'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_TIME_VERSION\n",
    "print(MODEL_TIME_VERSION)\n",
    "MODEL_TIME_VERSION_WITH_UNDERSCORES = str(MODEL_TIME_VERSION).replace('-','_')\n",
    "print(MODEL_TIME_VERSION_WITH_UNDERSCORES)\n",
    "MODEL_TIME_VERSION_WITHOUT_SPACE = str(MODEL_TIME_VERSION).replace('-','')\n",
    "print(MODEL_TIME_VERSION_WITHOUT_SPACE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### update VERSION in the cell below with the MODEL_TIME_VERSION above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile setup.py\n",
    "\n",
    "from setuptools import setup\n",
    "\n",
    "MODEL_NAME = \"tweet_sentiment_classifier\"\n",
    "REQUIRED_PACKAGES = ['gcsfs']\n",
    "VERSION = '2022-06-xx-xx-xx-xx'\n",
    "\n",
    "setup(\n",
    "    name=MODEL_NAME,\n",
    "    packages=[],\n",
    "    include_package_data=False,\n",
    "    version=VERSION,\n",
    "    scripts=[\"preprocess.py\", \"model_prediction.py\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrap it up and copy to GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/setuptools/dist.py:511: UserWarning: The version specified ('2022-06-05-13-38-45') is an invalid version, this may not work as expected with newer versions of setuptools, pip, and PyPI. Please see PEP 440 for more details.\n",
      "  \"details.\" % version\n",
      "running sdist\n",
      "running egg_info\n",
      "/opt/conda/lib/python3.7/site-packages/pkg_resources/__init__.py:119: PkgResourcesDeprecationWarning: 2022-06-05-13-38-45 is an invalid version and will not be supported in a future release\n",
      "  PkgResourcesDeprecationWarning,\n",
      "writing tweet_sentiment_classifier.egg-info/PKG-INFO\n",
      "writing dependency_links to tweet_sentiment_classifier.egg-info/dependency_links.txt\n",
      "writing top-level names to tweet_sentiment_classifier.egg-info/top_level.txt\n",
      "reading manifest file 'tweet_sentiment_classifier.egg-info/SOURCES.txt'\n",
      "writing manifest file 'tweet_sentiment_classifier.egg-info/SOURCES.txt'\n",
      "warning: sdist: standard file not found: should have one of README, README.rst, README.txt, README.md\n",
      "\n",
      "running check\n",
      "warning: check: missing required meta-data: url\n",
      "\n",
      "warning: check: missing meta-data: either (author and author_email) or (maintainer and maintainer_email) must be supplied\n",
      "\n",
      "creating tweet_sentiment_classifier-2022-06-05-13-38-45\n",
      "creating tweet_sentiment_classifier-2022-06-05-13-38-45/tweet_sentiment_classifier.egg-info\n",
      "copying files to tweet_sentiment_classifier-2022-06-05-13-38-45...\n",
      "copying model_prediction.py -> tweet_sentiment_classifier-2022-06-05-13-38-45\n",
      "copying preprocess.py -> tweet_sentiment_classifier-2022-06-05-13-38-45\n",
      "copying setup.py -> tweet_sentiment_classifier-2022-06-05-13-38-45\n",
      "copying tweet_sentiment_classifier.egg-info/PKG-INFO -> tweet_sentiment_classifier-2022-06-05-13-38-45/tweet_sentiment_classifier.egg-info\n",
      "copying tweet_sentiment_classifier.egg-info/SOURCES.txt -> tweet_sentiment_classifier-2022-06-05-13-38-45/tweet_sentiment_classifier.egg-info\n",
      "copying tweet_sentiment_classifier.egg-info/dependency_links.txt -> tweet_sentiment_classifier-2022-06-05-13-38-45/tweet_sentiment_classifier.egg-info\n",
      "copying tweet_sentiment_classifier.egg-info/top_level.txt -> tweet_sentiment_classifier-2022-06-05-13-38-45/tweet_sentiment_classifier.egg-info\n",
      "Writing tweet_sentiment_classifier-2022-06-05-13-38-45/setup.cfg\n",
      "Creating tar archive\n",
      "removing 'tweet_sentiment_classifier-2022-06-05-13-38-45' (and everything under it)\n",
      "Copying file://./dist/tweet_sentiment_classifier-2022-06-05-13-38-45.tar.gz [Content-Type=application/x-tar]...\n",
      "/ [1 files][  1.8 KiB/  1.8 KiB]                                                \n",
      "Operation completed over 1 objects/1.8 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!python setup.py sdist --formats=gztar\n",
    "!gsutil cp ./dist/{MODEL_NAME}-{MODEL_TIME_VERSION_WITHOUT_SPACE}.tar.gz {BUCKET}/{PACKAGES_DIR}/{MODEL_NAME}-{MODEL_TIME_VERSION_WITHOUT_SPACE}.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create model and version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "VERSION_NAME='V_' + MODEL_TIME_VERSION_WITHOUT_SPACE # MODEL_TIME_VERSION.replace(\"-\",\"_\")\n",
    "RUNTIME_VERSION='2.5' # tensorflow version\n",
    "MODEL_REGION='europe-west1'\n",
    "id_model = 'model-'+MODEL_TIME_VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If no model has been created before, run this command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://ml.googleapis.com/]\n",
      "\u001b[1;31mERROR:\u001b[0m (gcloud.ai-platform.models.create) Resource in projects [sound-splicer-351114] is the subject of a conflict: Field: model.name Error: A model with the same name already exists.\n",
      "- '@type': type.googleapis.com/google.rpc.BadRequest\n",
      "  fieldViolations:\n",
      "  - description: A model with the same name already exists.\n",
      "    field: model.name\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai-platform models create {MODEL_NAME} --regions {MODEL_REGION}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://ml.googleapis.com/]\n",
      "NAME                        DEFAULT_VERSION_NAME\n",
      "tweet_sentiment_classifier  V2\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai-platform models list --region global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://ml.googleapis.com/]\n",
      "Creating version (this might take a few minutes)......done.                    \n"
     ]
    }
   ],
   "source": [
    "!gcloud beta ai-platform versions create {VERSION_NAME} \\\n",
    "--model {MODEL_NAME} \\\n",
    "--origin {BUCKET}/{MODEL_DIR}/{id_model} \\\n",
    "--python-version 3.7 \\\n",
    "--runtime-version {RUNTIME_VERSION} \\\n",
    "--package-uris {BUCKET}/{PACKAGES_DIR}/{MODEL_NAME}-{MODEL_TIME_VERSION_WITHOUT_SPACE}.tar.gz \\\n",
    "--prediction-class=model_prediction.CustomModelPrediction \\\n",
    "--region global "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "requests = [\n",
    "    \"god this episode sucks\",\n",
    "    \"meh, I kinda like it\",\n",
    "    \"what were the writer thinking, omg it doesn't make any sense!\",\n",
    "    \"omg! what a twist, who would've though :o!\",\n",
    "    \"woohoow, sansa for the win!\"\n",
    "]\n",
    "\n",
    "# JSON format the requests\n",
    "request_data = {'instances': requests}\n",
    "\n",
    "# Authenticate and call CMLE prediction API \n",
    "credentials = GoogleCredentials.get_application_default()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.5 ms, sys: 1.51 ms, total: 22 ms\n",
      "Wall time: 66.1 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'negative', 'score': 0.050200000405311584},\n",
       " {'label': 'positive', 'score': 0.7918000221252441},\n",
       " {'label': 'negative', 'score': 0.373199999332428},\n",
       " {'label': 'negative', 'score': 0.193900004029274},\n",
       " {'label': 'positive', 'score': 0.8440999984741211}]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "api = discovery.build('ml', 'v1')\n",
    "model_url = 'projects/{}/models/{}'.format(PROJECT, MODEL_NAME)\n",
    "response = api.projects().predict(body=request_data, name=model_url).execute()\n",
    "response[\"predictions\"]"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-3.m93",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m93"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
